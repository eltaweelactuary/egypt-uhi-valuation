# %% [markdown]
# # ğŸ† Ø­Ù„ Ù…Ø³Ø§Ø¨Ù‚Ø© Actuarial Loss Estimation
# ## Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªÙƒØ§Ù„ÙŠÙ Ù…Ø·Ø§Ù„Ø¨Ø§Øª ØªØ¹ÙˆÙŠØ¶Ø§Øª Ø§Ù„Ø¹Ù…Ø§Ù„
# 
# Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙŠØ­Ù„ Ù…Ø³Ø§Ø¨Ù‚Ø© Kaggle Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨ØªÙƒÙ„ÙØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ø§Ù„ØªØ£Ù…ÙŠÙ†ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©

# %% [markdown]
# ## ğŸ“¦ Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ«Ø¨ÙŠØª ÙˆØ¥Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª

# %%
# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
!pip install -q kaggle xgboost lightgbm catboost

# %%
# Ø¥Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor

print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")

# %% [markdown]
# ## ğŸ“¥ Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kaggle

# %%
# Ø¥Ø¹Ø¯Ø§Ø¯ Kaggle API
# Ø§Ø±ÙØ¹ Ù…Ù„Ù kaggle.json Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
from google.colab import files
print("ğŸ“¤ Ø§Ø±ÙØ¹ Ù…Ù„Ù kaggle.json:")
# files.upload()  # Ù‚Ù… Ø¨Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù

# %%
# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯ Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/ 2>/dev/null || echo "ØªØ£ÙƒØ¯ Ù…Ù† Ø±ÙØ¹ kaggle.json"
!chmod 600 ~/.kaggle/kaggle.json

# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©
!kaggle competitions download -c actuarial-loss-estimation
!unzip -q actuarial-loss-estimation.zip -d data/

print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")

# %% [markdown]
# ## ğŸ“Š Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (EDA)

# %%
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

print(f"ğŸ“ˆ Ø­Ø¬Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train.shape}")
print(f"ğŸ“‰ Ø­Ø¬Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test.shape}")
print(f"\nğŸ¯ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù: UltimateIncurredClaimCost")

# %%
# Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ
print("ğŸ“‹ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
train.head()

# %%
# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
print("ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:")
train.info()

# %%
# Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØµÙÙŠØ©
print("ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙÙŠØ©:")
train.describe()

# %%
# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(train['UltimateIncurredClaimCost'], bins=50, color='steelblue', edgecolor='white')
axes[0].set_title('ØªÙˆØ²ÙŠØ¹ ØªÙƒÙ„ÙØ© Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª', fontsize=14)
axes[0].set_xlabel('Ø§Ù„ØªÙƒÙ„ÙØ©')

axes[1].hist(np.log1p(train['UltimateIncurredClaimCost']), bins=50, color='coral', edgecolor='white')
axes[1].set_title('ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙƒÙ„ÙØ© (Ø¨Ø¹Ø¯ Log Transform)', fontsize=14)
axes[1].set_xlabel('log(Ø§Ù„ØªÙƒÙ„ÙØ© + 1)')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## ğŸ”§ Ø§Ù„Ø®Ø·ÙˆØ© 4: Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Feature Engineering)

# %%
def create_features(df, is_train=True):
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    df = df.copy()
    
    # 1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
    date_cols = ['DateTimeOfAccident', 'DateReported']
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            df[f'{col}_Year'] = df[col].dt.year
            df[f'{col}_Month'] = df[col].dt.month
            df[f'{col}_DayOfWeek'] = df[col].dt.dayofweek
    
    # 2. ØªØ£Ø®ÙŠØ± Ø§Ù„Ø¥Ø¨Ù„Ø§Øº (Ø¨Ø§Ù„Ø£ÙŠØ§Ù…)
    if 'DateReported' in df.columns and 'DateTimeOfAccident' in df.columns:
        df['ReportingDelay'] = (df['DateReported'] - df['DateTimeOfAccident']).dt.days
    
    # 3. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø§ØªØ¨
    if 'WeeklyWages' in df.columns:
        df['WeeklyWages_Log'] = np.log1p(df['WeeklyWages'])
        df['AnnualWages'] = df['WeeklyWages'] * 52
    
    # 4. Ù…ÙŠØ²Ø§Øª Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„
    if 'HoursWorkedPerWeek' in df.columns:
        df['IsPartTime'] = (df['HoursWorkedPerWeek'] < 35).astype(int)
        if 'WeeklyWages' in df.columns:
            df['HourlyWage'] = df['WeeklyWages'] / df['HoursWorkedPerWeek'].replace(0, 1)
    
    # 5. Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ (ÙˆØµÙ Ø§Ù„Ø­Ø§Ø¯Ø«)
    text_col = None
    for col in ['ClaimDescription', 'AccidentDescription']:
        if col in df.columns:
            text_col = col
            break
    
    if text_col:
        df[text_col] = df[text_col].fillna('')
        df['TextLength'] = df[text_col].apply(len)
        df['WordCount'] = df[text_col].apply(lambda x: len(str(x).split()))
        
        # ÙƒÙ„Ù…Ø§Øª ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø´Ø¯Ø© Ø§Ù„Ø¥ØµØ§Ø¨Ø©
        severity_words = ['severe', 'serious', 'fracture', 'surgery', 'hospital', 'permanent']
        df['SeverityScore'] = df[text_col].apply(
            lambda x: sum(1 for w in severity_words if w.lower() in str(x).lower())
        )
    
    return df

# ØªØ·Ø¨ÙŠÙ‚ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª
train = create_features(train, is_train=True)
test = create_features(test, is_train=False)

print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©!")
print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¹Ø¯ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª: {train.shape[1]}")

# %% [markdown]
# ## ğŸ·ï¸ Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬

# %%
# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
TARGET = 'UltimateIncurredClaimCost'
y = train[TARGET]
y_log = np.log1p(y)  # ØªØ­ÙˆÙŠÙ„ Ù„ÙˆØºØ§Ø±ÙŠØªÙ…ÙŠ Ù„Ù„ØªÙˆØ²ÙŠØ¹

# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
EXCLUDE_COLS = [TARGET, 'ClaimNumber', 'ClaimDescription', 'AccidentDescription',
                'DateTimeOfAccident', 'DateReported', 'DateOfBirth']

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
feature_cols = [c for c in train.columns if c not in EXCLUDE_COLS]
print(f"ğŸ“‹ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª: {len(feature_cols)}")

# %%
# ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
label_encoders = {}
categorical_cols = train[feature_cols].select_dtypes(include=['object']).columns.tolist()

for col in categorical_cols:
    le = LabelEncoder()
    train[col] = train[col].fillna('MISSING').astype(str)
    test[col] = test[col].fillna('MISSING').astype(str)
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙˆØ­Ø¯
    all_values = pd.concat([train[col], test[col]]).unique()
    le.fit(all_values)
    
    train[col] = le.transform(train[col])
    test[col] = le.transform(test[col])
    label_encoders[col] = le

print(f"âœ… ØªÙ… ØªØ±Ù…ÙŠØ² {len(categorical_cols)} Ø¹Ù…ÙˆØ¯ ÙØ¦ÙˆÙŠ")

# %%
# ØªØ­Ø¶ÙŠØ± Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X = train[feature_cols].fillna(-999)
X_test = test[feature_cols].fillna(-999)

print(f"ğŸ“Š Ø´ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {X.shape}")
print(f"ğŸ“Š Ø´ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {X_test.shape}")

# %% [markdown]
# ## ğŸ¤– Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬

# %%
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
N_FOLDS = 5
SEED = 42

# ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
oof_xgb = np.zeros(len(X))
oof_lgb = np.zeros(len(X))
oof_cat = np.zeros(len(X))

pred_xgb = np.zeros(len(X_test))
pred_lgb = np.zeros(len(X_test))
pred_cat = np.zeros(len(X_test))

# %%
# ØªØ¯Ø±ÙŠØ¨ XGBoost
print("ğŸš€ ØªØ¯Ø±ÙŠØ¨ XGBoost...")

kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"  Fold {fold+1}/{N_FOLDS}", end=" ")
    
    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_tr, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]
    
    model = xgb.XGBRegressor(
        n_estimators=1000, max_depth=6, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, random_state=SEED,
        early_stopping_rounds=50, eval_metric='mae', verbosity=0
    )
    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
    
    oof_xgb[val_idx] = model.predict(X_val)
    pred_xgb += model.predict(X_test) / N_FOLDS
    
    mae = mean_absolute_error(y_val, oof_xgb[val_idx])
    print(f"MAE: {mae:.4f}")

xgb_score = mean_absolute_error(y_log, oof_xgb)
print(f"âœ… XGBoost OOF MAE: {xgb_score:.4f}")

# %%
# ØªØ¯Ø±ÙŠØ¨ LightGBM
print("\nğŸš€ ØªØ¯Ø±ÙŠØ¨ LightGBM...")

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"  Fold {fold+1}/{N_FOLDS}", end=" ")
    
    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_tr, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]
    
    model = lgb.LGBMRegressor(
        n_estimators=1000, max_depth=6, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, random_state=SEED,
        verbose=-1
    )
    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],
              callbacks=[lgb.early_stopping(50, verbose=False)])
    
    oof_lgb[val_idx] = model.predict(X_val)
    pred_lgb += model.predict(X_test) / N_FOLDS
    
    mae = mean_absolute_error(y_val, oof_lgb[val_idx])
    print(f"MAE: {mae:.4f}")

lgb_score = mean_absolute_error(y_log, oof_lgb)
print(f"âœ… LightGBM OOF MAE: {lgb_score:.4f}")

# %%
# ØªØ¯Ø±ÙŠØ¨ CatBoost
print("\nğŸš€ ØªØ¯Ø±ÙŠØ¨ CatBoost...")

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"  Fold {fold+1}/{N_FOLDS}", end=" ")
    
    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_tr, y_val = y_log.iloc[train_idx], y_log.iloc[val_idx]
    
    model = CatBoostRegressor(
        iterations=1000, depth=6, learning_rate=0.05,
        random_state=SEED, verbose=0
    )
    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=50)
    
    oof_cat[val_idx] = model.predict(X_val)
    pred_cat += model.predict(X_test) / N_FOLDS
    
    mae = mean_absolute_error(y_val, oof_cat[val_idx])
    print(f"MAE: {mae:.4f}")

cat_score = mean_absolute_error(y_log, oof_cat)
print(f"âœ… CatBoost OOF MAE: {cat_score:.4f}")

# %% [markdown]
# ## ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© 7: Ø¥Ù†Ø´Ø§Ø¡ Ensemble ÙˆÙ…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…

# %%
# Ø¯Ù…Ø¬ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ensemble)
oof_ensemble = (oof_xgb + oof_lgb + oof_cat) / 3
pred_ensemble = (pred_xgb + pred_lgb + pred_cat) / 3

ensemble_score = mean_absolute_error(y_log, oof_ensemble)
print(f"ğŸ† Ensemble OOF MAE: {ensemble_score:.4f}")

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù„Ù„Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£ØµÙ„ÙŠ
final_predictions = np.expm1(pred_ensemble)
final_predictions = np.maximum(final_predictions, 0)  # Ù„Ø§ Ù‚ÙŠÙ… Ø³Ø§Ù„Ø¨Ø©

# %%
# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…
submission = pd.DataFrame({
    'ClaimNumber': test['ClaimNumber'],
    'UltimateIncurredClaimCost': final_predictions
})

submission.to_csv('submission.csv', index=False)
print("âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…: submission.csv")
submission.head()

# %%
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
from google.colab import files
files.download('submission.csv')
print("ğŸ“¥ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…!")

# %% [markdown]
# ## ğŸ“ˆ Ø§Ù„Ø®Ø·ÙˆØ© 8: ØªØ­Ù„ÙŠÙ„ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª

# %%
# Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø¢Ø®Ø± Ù†Ù…ÙˆØ°Ø¬ LightGBM
importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(importance['feature'][:20], importance['importance'][:20], color='steelblue')
plt.xlabel('Ø§Ù„Ø£Ù‡Ù…ÙŠØ©')
plt.title('Ø£Ù‡Ù… 20 Ù…ÙŠØ²Ø©')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# %% [markdown]
# ---
# ## ğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙƒÙˆØ¯!
# 
# ### Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:
# - ØªÙ… ØªØ¯Ø±ÙŠØ¨ 3 Ù†Ù…Ø§Ø°Ø¬: XGBoost, LightGBM, CatBoost
# - ØªÙ… Ø¯Ù…Ø¬Ù‡Ø§ ÙÙŠ Ensemble Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
# - Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø±ÙØ¹ Ø¹Ù„Ù‰ Kaggle
# 
# ### Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø©:
# 1. Ø¬Ø±Ø¨ hyperparameter tuning
# 2. Ø£Ø¶Ù Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù†Øµ (TF-IDF, embeddings)
# 3. Ø¬Ø±Ø¨ Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ø±Ù‰ (Neural Networks)
